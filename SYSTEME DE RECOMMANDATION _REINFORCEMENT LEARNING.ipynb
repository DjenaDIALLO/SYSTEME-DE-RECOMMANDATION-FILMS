{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ef03c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/djenadiallo/anaconda3/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/djenadiallo/anaconda3/lib/python3.12/site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/djenadiallo/anaconda3/lib/python3.12/site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /home/djenadiallo/anaconda3/lib/python3.12/site-packages (from gym) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    " !pip install gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351f05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "ratings_df = pd.read_csv('ratings.csv')\n",
    "movies_df = pd.read_csv('movies.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "556e69ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10324</th>\n",
       "      <td>146684</td>\n",
       "      <td>Cosmic Scrat-tastrophe (2015)</td>\n",
       "      <td>Animation|Children|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10325</th>\n",
       "      <td>146878</td>\n",
       "      <td>Le Grand Restaurant (1966)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10326</th>\n",
       "      <td>148238</td>\n",
       "      <td>A Very Murray Christmas (2015)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10327</th>\n",
       "      <td>148626</td>\n",
       "      <td>The Big Short (2015)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10328</th>\n",
       "      <td>149532</td>\n",
       "      <td>Marco Polo: One Hundred Eyes (2015)</td>\n",
       "      <td>(no genres listed)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10329 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       movieId                                title  \\\n",
       "0            1                     Toy Story (1995)   \n",
       "1            2                       Jumanji (1995)   \n",
       "2            3              Grumpier Old Men (1995)   \n",
       "3            4             Waiting to Exhale (1995)   \n",
       "4            5   Father of the Bride Part II (1995)   \n",
       "...        ...                                  ...   \n",
       "10324   146684        Cosmic Scrat-tastrophe (2015)   \n",
       "10325   146878           Le Grand Restaurant (1966)   \n",
       "10326   148238       A Very Murray Christmas (2015)   \n",
       "10327   148626                 The Big Short (2015)   \n",
       "10328   149532  Marco Polo: One Hundred Eyes (2015)   \n",
       "\n",
       "                                            genres  \n",
       "0      Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                       Adventure|Children|Fantasy  \n",
       "2                                   Comedy|Romance  \n",
       "3                             Comedy|Drama|Romance  \n",
       "4                                           Comedy  \n",
       "...                                            ...  \n",
       "10324                    Animation|Children|Comedy  \n",
       "10325                                       Comedy  \n",
       "10326                                       Comedy  \n",
       "10327                                        Drama  \n",
       "10328                           (no genres listed)  \n",
       "\n",
       "[10329 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb420e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1217897793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1217895807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1217896246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1217896556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1217896523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105334</th>\n",
       "      <td>668</td>\n",
       "      <td>142488</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1451535844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105335</th>\n",
       "      <td>668</td>\n",
       "      <td>142507</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1451535889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105336</th>\n",
       "      <td>668</td>\n",
       "      <td>143385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1446388585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105337</th>\n",
       "      <td>668</td>\n",
       "      <td>144976</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1448656898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105338</th>\n",
       "      <td>668</td>\n",
       "      <td>148626</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1451148148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105339 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp\n",
       "0            1       16     4.0  1217897793\n",
       "1            1       24     1.5  1217895807\n",
       "2            1       32     4.0  1217896246\n",
       "3            1       47     4.0  1217896556\n",
       "4            1       50     4.0  1217896523\n",
       "...        ...      ...     ...         ...\n",
       "105334     668   142488     4.0  1451535844\n",
       "105335     668   142507     3.5  1451535889\n",
       "105336     668   143385     4.0  1446388585\n",
       "105337     668   144976     2.5  1448656898\n",
       "105338     668   148626     4.5  1451148148\n",
       "\n",
       "[105339 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffcf12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MovieLensEnv(gym.Env):\n",
    "    \"\"\"\n",
    "     Environnement personnalisé qui utilise gym interface et simule\n",
    "    un système de recommandation basé sur l'ensemble de données MovieLens.\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['console']}\n",
    "    \n",
    "    def __init__(self, user_profiles, movie_list):\n",
    "        super(MovieLensEnv, self).__init__()\n",
    "        # Définir les espaces d'action et d'observation\n",
    "        # Ils doivent être des objets gym.spaces\n",
    "        # Exemple d'utilisation d'actions discrètes, nous avons une action pour chaque film\n",
    "        self.action_space = spaces.Discrete(len(movie_list))\n",
    "        # L'observation sera le profil de l'utilisateur, qui pourrait inclure\n",
    "        # des données démographiques sur l'utilisateur et des évaluations de films\n",
    "        self.observation_space = spaces.Box(low=0, high=5, shape=(len(user_profiles[0]),), dtype=np.float32)\n",
    "        \n",
    "        self.user_profiles = user_profiles\n",
    "        self.movie_list = movie_list\n",
    "        self.current_user = 0  # Garder trace de l'index de l'utilisateur actuel\n",
    "\n",
    "    def step(self, action):\n",
    "        # 'action' est le movie ID du film à recommander\n",
    "        current_profile = self.user_profiles[self.current_user]\n",
    "\n",
    "        #Trouver l'index dans 'self.movie_list' qui correspond à movie ID dans 'action'\n",
    "        try:\n",
    "            action_index = np.where(self.movie_list == action)[0][0]\n",
    "        except IndexError:\n",
    "            raise ValueError(f\"Movie ID {action} is not found in the movie list.\")\n",
    "    \n",
    "        # Simuler l'action de notation de l'utilisateur à l'aide de l'index\n",
    "        reward, done = self.simulate_user_rating(current_profile, action_index)\n",
    "        self.current_user = (self.current_user + 1) % len(self.user_profiles)\n",
    "        next_state = self.user_profiles[self.current_user]\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "         #Réinitialiser l'état de l'environnement à un état initial\n",
    "        self.current_user = 0\n",
    "        return self.user_profiles[self.current_user]  # renvoie le premier profil d'utilisateur\n",
    "    \n",
    "    def render(self, mode='console'):\n",
    "        if mode != 'console':\n",
    "            raise NotImplementedError()\n",
    "        # Pour simplifier,on va juste afficher le profil et l'action de l'utilisateur actuel.\n",
    "        print(f\"User Profile: {self.user_profiles[self.current_user]}\")\n",
    "    \n",
    "    def simulate_user_rating(self, user_profile, recommended_movie_id):\n",
    "        # Vérifie si l'utilisateur a noté le film\n",
    "        user_ratings = ratings_df[ratings_df['userId'] == self.current_user]\n",
    "        if recommended_movie_id in user_ratings['movieId'].values:\n",
    "            # Si l'utilisateur a noté le film, on utilise cette note comme recompense\n",
    "            reward = user_ratings[user_ratings['movieId'] == recommended_movie_id]['rating'].iloc[0]\n",
    "            done = True  \n",
    "        else:\n",
    "            # si l'utilisateur n'a pas noté le film, on lui donne une recompense neutre qui peut etre la note moyenne de l'utilisateur\n",
    "           \n",
    "            reward = user_ratings['rating'].mean() if not user_ratings.empty else 2.5\n",
    "            done = False  \n",
    "            \n",
    "        # Ici, nous pourrions également mettre en œuvre un mécanisme pour estimer la note si le film n'a pas été noté\n",
    "        # Par exemple, en utilisant un modèle de filtrage collaboratif ou basé sur le contenu pour prédire la note.\n",
    "        \n",
    "        return reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a76641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Profile Vector for User ID 1:\n",
      "[3.62831858 3.62831858 3.62831858 ... 3.62831858 3.62831858 3.62831858]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_id = 1  #on peut le remplacer par n'importe quel autre user_Id\n",
    "# creer une liste de tous les movie IDs\n",
    "all_movie_ids = movies_df['movieId'].unique()\n",
    "#Initialiser le profil de l'utilisateur avec des zéros pour chaque film\n",
    "user_profile = np.zeros(len(all_movie_ids))\n",
    "\n",
    "# Obtenir toutes les notes de l'utilisateur\n",
    "user_ratings = ratings_df[ratings_df['userId'] == user_id]\n",
    "\n",
    "# Mise à jour du profil de l'utilisateur avec les notes de l'utilisateur\n",
    "for movie_id, rating in user_ratings[['movieId', 'rating']].values:\n",
    "    movie_index = np.where(all_movie_ids == movie_id)[0][0]  # Find the index of the movie_id in all_movie_ids\n",
    "    user_profile[movie_index] = rating\n",
    "\n",
    "# Imputer les films non notés avec la note moyenne de l'utilisateur\n",
    "average_rating = user_ratings['rating'].mean()\n",
    "user_profile[user_profile == 0] = average_rating  \n",
    "\n",
    "print(f\"User Profile Vector for User ID {user_id}:\")\n",
    "print(user_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "710330d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State (User Profile): [3.62831858 3.62831858 3.62831858 ... 3.62831858 3.62831858 3.62831858]\n",
      "Next State (User Profile): [3.62831858 3.62831858 3.62831858 ... 3.62831858 3.62831858 3.62831858]\n",
      "Reward: 2.5\n",
      "Done: False\n"
     ]
    }
   ],
   "source": [
    "# Maintenant, nous allons créer une collection de profils d'utilisateurs pour les besoins de cet exemple\n",
    "# Pour plus de simplicité, nous allons simplement dupliquer le profil de cet utilisateur\n",
    "# On aurait également pu creer des profils distincts pour des utilisateur différents\n",
    "user_profiles = np.array([user_profile for _ in range(10)])  # Replace with your actual user profiles data\n",
    "\n",
    "# On lance une instance MovieLensEnv avec une collection de user_profiles\n",
    "env = MovieLensEnv(user_profiles, all_movie_ids)\n",
    "\n",
    "# Réinitialiser l'environnement \n",
    "initial_state = env.reset()\n",
    "\n",
    "print(f\"Initial State (User Profile): {initial_state}\")\n",
    "\n",
    "action_index = np.random.choice(len(all_movie_ids))\n",
    "\n",
    "\n",
    "movie_id_to_recommend = all_movie_ids[action_index]\n",
    "next_state, reward, done, info = env.step(movie_id_to_recommend)\n",
    "\n",
    "print(f\"Next State (User Profile): {next_state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# Le rendu de l'environnement peut montrer le profil de l'utilisateur actuel et le film recommandé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d3f11d",
   "metadata": {},
   "source": [
    "  Etat Initial (User Profile): Il s'agit de la représentation de l'état du profil de l'utilisateur au début d'un épisode. Les chiffres qu'ont voit 3.62831858 répétés tout au long du vecteur, sont la note moyenne donnée par l'utilisateur à tous les films notés ou une note moyenne par défaut pour les films non notés. Ce vecteur sert d'entrée à notre agent RL qui l'utilisera pour décider quel film recommander.\n",
    "\n",
    "   État suivant (Next State): Une fois que l'agent a effectué une action (recommandé un film), l'environnement passe à l'état suivant. Puisqu'on itère sur les utilisateurs en séquence, cet état suivant est le profil de l'utilisateur suivant dans notre ensemble de données. Le profil comporte les mêmes valeurs de remplacement, indiquant soit que l'utilisateur suivant a une note moyenne similaire à celle de l'utilisateur initial, soit qu'on utilise une note moyenne par défaut pour les films non notés.\n",
    "\n",
    "  Récompense (Reward): L'agent a reçu une récompense de 2,5 pour l'action entreprise. Dans le contexte d'un système de recommandation de films, cette récompense pourrait représenter une évaluation neutre, indiquant éventuellement que le film recommandé n'a été évalué ni positivement ni négativement par l'utilisateur, ou que le film n'avait pas été évalué par l'utilisateur auparavant et qu'on utilise une valeur neutre comme récompense.\n",
    "\n",
    "  Terminé (Done):Cette valeur booléenne indique si l'épisode est terminé. La valeur False signifie que l'épisode est toujours en cours. Dans notre configuration d'apprentissage par renforcement, un épisode peut représenter une séquence de recommandations pour un utilisateur ou un ensemble d'utilisateurs.\n",
    "\n",
    "  Profil d'utilisateur rendu (Rendered User Profile): Il s'agit apparemment du même état que l'état initial, produit dans le cadre d'une fonction de rendu. La fonction de rendu est généralement utilisée pour visualiser ou produire l'état actuel de l'environnement sous une forme lisible par l'homme. Elle montre le profil de l'utilisateur actuel auquel l'agent a fait une recommandation.\n",
    "\n",
    "En résumé, l'agent RL interagit avec l'environnement en recommandant des films aux utilisateurs. L'état représente les profils des utilisateurs, l'action est une recommandation de film, la récompense reflète la satisfaction de l'utilisateur et le « done » indique la fin d'une séquence d'interaction. Chaque étape franchie par l'agent lui donne des informations sur l'efficacité de ses recommandations, qui seront utilisées pour améliorer sa politique au fil du temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef457b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_value(Q_table, state, action, reward, next_state, alpha, gamma, possible_actions):\n",
    "    #S'assurer que l'état et l'état suivant sont des tuples afin qu'ils puissent être utilisés comme dictionary keys\n",
    "    state_key = tuple(state)\n",
    "    next_state_key = tuple(next_state)\n",
    "    \n",
    "    #Calculer la valeur maximale q-value pour les actions dans l'état suivant\n",
    "    max_future_q = max(Q_table.get((next_state_key, a), 0) for a in possible_actions)  # Use a default Q-value of 0 if not found\n",
    "    \n",
    "    # Calculer la valeur actuelle q-value\n",
    "    current_q = Q_table.get((state_key, action), 0)  # if the state-action pair is new, assume a default Q-value of 0\n",
    "    \n",
    "    # Calculer la nouvelle valeur q-value\n",
    "    new_q = (1 - alpha) * current_q + alpha * (reward + gamma * max_future_q)\n",
    "    \n",
    "    # Mettre à jour Q-table avec la nouvelle valeur de q-value\n",
    "    Q_table[(state_key, action)] = new_q\n",
    "    return new_q  # Le renvoi de la nouvelle valeur q-value qui peut être utile à des fins de logging ou de debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "267a3f6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305cf5f95038407f913c8ddd13a93d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 293.800986377622\n",
      "Episode 2: Total Reward = 159.6557247529296\n",
      "Episode 3: Total Reward = 1098.529108361166\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Defir les hyperparamètres\n",
    "num_episodes = 3  # Le nombre d'épisodes pour lesquels la simulation doit être effectuée\n",
    "learning_rate = 0.1  # Le taux d'apprentissage pour la mise à jour du  Q-learning \n",
    "discount_factor = 0.99  # Le facteur d'actualisation des récompenses futures\n",
    "\n",
    "# Initialiser Q-table \n",
    "#Pour simplifier, il peut s'agir d'une table ou d'un réseau de neurone en fonction de la complexité.\n",
    "Q_table = {}\n",
    "possible_actions = list(range(len(env.movie_list)))\n",
    "# Boucle d'apprentissage\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()  # Démarrer un nouvel épisode et obtenir l'état initial\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        #Choisir une action en fonction de la politique actuelle\n",
    "        #  Pour l'instant, nous supposons une politique aléatoire\n",
    "        action = np.random.choice(env.movie_list)\n",
    "        \n",
    "        # Prendre l'action et observer le prochain état et la prochaine récompense\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state_tuple = tuple(state)\n",
    "        next_state_tuple = tuple(next_state)\n",
    "\n",
    "        update_q_value(Q_table, state_tuple, action, reward, next_state_tuple, learning_rate, discount_factor, possible_actions)\n",
    "       \n",
    "        #Cumuler les récompenses\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Passage à l'état suivant\n",
    "        state = next_state\n",
    "    \n",
    "    # Enregistrez la récompense totale pour cet épisode\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    # En option, on peut ajouter du code pour évaluer la politique ici et interrompre la boucle plus tôt si une certaine performance est atteinte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4991964d",
   "metadata": {},
   "source": [
    " p.s. \n",
    "\n",
    "La Q-table est une matrice dans laquelle il y a une ligne pour chaque état et une colonne pour chaque action. Elle est d'abord initialisée à 0 puis les valeurs sont mises à jour après l'entraînement. La Q-table a les mêmes dimensions que la reward table, mais elle a un objectif complètement différent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#  Après l'entrainement,on peut enregistrer notre modèle ou notre Q-table pour un usage ultérieur.\n",
    "#with open('q_table.pickle', 'wb') as handle:\n",
    "    #pickle.dump(Q_table, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994555c",
   "metadata": {},
   "source": [
    "En deep Q-learning, on remplace la Q-table par un réseau neuronal qui approxime les valeurs Q-values  pour chaque paire état-action..\n",
    "\n",
    "Voici une vue d'ensemble de ce que nous allons faire :\n",
    "\n",
    "Définir une architecture de réseau neuronal Q-network.\n",
    "Remplacer la recherche et la mise à jour dans la Q-table par des passages avant et arrière dans le réseau neuronal.\n",
    "Implementer e renouvellement de l'expérience afin d'échantillonner de manière aléatoire les expériences pour l'entrainement du  Q-network.\n",
    "Utiliser un réseau cible pour stabiliser l'entrainement,qui est périodiquement mis à jour avec les poids du Q-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de5d1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Définir l'architecture Q-Network \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialiser le Q-Network et le réseau cible\n",
    "state_size = len(state)  # TLa taille de la représentation de l'État\n",
    "action_size = len(possible_actions)  # Le nombre d'actions possibles\n",
    "q_network = QNetwork(state_size, action_size)\n",
    "target_network = QNetwork(state_size, action_size)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# La fonction de mise à jour prend désormais en compte un ensemble d'expériences\n",
    "def update_q_network(batch, gamma, alpha):\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    next_states = torch.FloatTensor(next_states)\n",
    "    dones = torch.ByteTensor(dones)\n",
    "\n",
    "    # Obtenir les Q values pour les états actuels\n",
    "    Q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    #Calculer les  Q values pour les états suivants en utilisant le réseau cible\n",
    "    Q_next = target_network(next_states).detach()\n",
    "    Q_next_values = Q_next.max(1)[0]\n",
    "    Q_targets = rewards + gamma * Q_next_values * (1 - dones)\n",
    "    \n",
    "    #  Calculer la perte\n",
    "    loss = criterion(Q_values, Q_targets)\n",
    "    \n",
    "    #  Optimiser le modèle\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Optionnellement, mettre à jour le réseau cible avec les poids du  Q-network's\n",
    "    # Cela pourrait se faire à intervalles réguliers, pas nécessairement à chaque mise à jour.\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "# Experience replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Exemple d'usage\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "batch_size = 32\n",
    "\n",
    "#  Supposons que nous disposions d'un moyen d'obtenir des expériences et de les stocker dans replay buffer\n",
    "# Pour state, action, reward, next_state, done dans les expériences:\n",
    "#     replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "# Entraînement du réseau\n",
    "if len(replay_buffer) > batch_size:\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    update_q_network(batch, gamma, alpha)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
